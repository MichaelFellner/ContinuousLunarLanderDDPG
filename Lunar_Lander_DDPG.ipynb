{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "th.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "class OUNoise:\n",
    "    def __init__(self, action_dim, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu * np.ones(action_dim)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.copy(self.mu)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.mu)\n",
    "    \n",
    "    def sample(self):\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state += dx\n",
    "        return self.state\n",
    "\n",
    "class HoverLunarLander(gym.Wrapper):\n",
    "    def __init__(self, env, target_location=(0.1, 0.25), epsilon=1e-3, \n",
    "                 penalty_landing=-50.0, penalty_crashing=-100.0, penalty_offscreen=-100.0):\n",
    "        super(HoverLunarLander, self).__init__(env)\n",
    "        self.target_x = target_location[0]\n",
    "        self.target_y = target_location[1]\n",
    "        self.epsilon = epsilon\n",
    "        self.penalty_landing = penalty_landing\n",
    "        self.penalty_crashing = penalty_crashing\n",
    "        self.penalty_offscreen = penalty_offscreen\n",
    "\n",
    "    def step(self, action):\n",
    "        state, original_reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        x, y = state[0], state[1]\n",
    "        \n",
    "        # Compute distance to target location\n",
    "        distance = math.sqrt((x - self.target_x)**2 + (y - self.target_y)**2)\n",
    "        \n",
    "        # Compute inverse distance reward (commented out to use normal reward)\n",
    "        inverse_distance_reward = original_reward#1.0 / (distance + self.epsilon)\n",
    "        new_reward = inverse_distance_reward\n",
    "        \n",
    "        if terminated:\n",
    "            if self.is_landed(state):\n",
    "                new_reward += self.penalty_landing\n",
    "                info['termination_cause'] = 'landed'\n",
    "            else:\n",
    "                new_reward += self.penalty_crashing\n",
    "                info['termination_cause'] = 'crashed'\n",
    "        elif truncated:\n",
    "            # Went off screen\n",
    "            new_reward += self.penalty_offscreen\n",
    "            info['termination_cause'] = 'offscreen'\n",
    "        \n",
    "        return state, new_reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def is_landed(self, state):\n",
    "        # Criteria for successful landing:\n",
    "        # 1. Both legs are in contact (state[6] and state[7] == 1)\n",
    "        # 2. Vertical and horizontal velocities are low\n",
    "        # 3. Angle is near vertical\n",
    "        leg_contact = state[6] == 1 and state[7] == 1\n",
    "        vertical_velocity = abs(state[3]) < 0.5  # state[3] is y-velocity\n",
    "        horizontal_velocity = abs(state[2]) < 0.5  # state[2] is x-velocity\n",
    "        angle = abs(state[4]) < 0.1  # state[4] is angle\n",
    "        \n",
    "        return leg_contact and vertical_velocity and horizontal_velocity and angle\n",
    "\n",
    "# Actor Network with Layer Normalization\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.max_action = max_action\n",
    "        \n",
    "        self.layer1 = nn.Linear(state_dim, 400)\n",
    "        self.ln1 = nn.LayerNorm(400)\n",
    "        self.layer2 = nn.Linear(400, 300)\n",
    "        self.ln2 = nn.LayerNorm(300)\n",
    "        self.layer3 = nn.Linear(300, 200)\n",
    "        self.ln3 = nn.LayerNorm(200)\n",
    "        self.layer4 = nn.Linear(200, action_dim)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.output_activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.activation(self.ln1(self.layer1(state)))\n",
    "        x = self.activation(self.ln2(self.layer2(x)))\n",
    "        x = self.activation(self.ln3(self.layer3(x)))\n",
    "        x = self.output_activation(self.layer4(x))\n",
    "        return x * self.max_action\n",
    "\n",
    "# Critic Network with Layer Normalization\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.ln1 = nn.LayerNorm(400)\n",
    "        self.layer2 = nn.Linear(400, 300)\n",
    "        self.ln2 = nn.LayerNorm(300)\n",
    "        self.layer3 = nn.Linear(300, 200)\n",
    "        self.ln3 = nn.LayerNorm(200)\n",
    "        self.layer4 = nn.Linear(200, 1)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = th.cat([state, action], dim=1)\n",
    "        x = self.activation(self.ln1(self.layer1(x)))\n",
    "        x = self.activation(self.ln2(self.layer2(x)))\n",
    "        x = self.activation(self.ln3(self.layer3(x)))\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=1000000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# DDPG Agent with Flexible Initialization\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action, device, actor=None, critic=None):\n",
    "        \"\"\"\n",
    "        Initializes the DDPG Agent.\n",
    "        \n",
    "        Args:\n",
    "            state_dim (int): Dimension of the state space.\n",
    "            action_dim (int): Dimension of the action space.\n",
    "            max_action (float): Maximum action value.\n",
    "            device (torch.device): Device to run the networks on.\n",
    "            actor (nn.Module, optional): Preloaded Actor network. Defaults to None.\n",
    "            critic (nn.Module, optional): Preloaded Critic network. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.max_action = max_action\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.batch_size = 64\n",
    "        self.grad_clip = 1.0  # Gradient clipping value\n",
    "        \n",
    "        # Initialize Actor Network\n",
    "        if actor is not None:\n",
    "            self.actor = actor.to(device)\n",
    "            print(\"Preloaded Actor network loaded.\")\n",
    "        else:\n",
    "            self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "            print(\"New Actor network initialized.\")\n",
    "        \n",
    "        # Initialize Actor Target Network\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        if actor is not None:\n",
    "            self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "            print(\"Actor target network initialized with preloaded Actor weights.\")\n",
    "        else:\n",
    "            self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "            print(\"Actor target network initialized with Actor weights.\")\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "        \n",
    "        # Initialize Critic Network\n",
    "        if critic is not None:\n",
    "            self.critic = critic.to(device)\n",
    "            print(\"Preloaded Critic network loaded.\")\n",
    "        else:\n",
    "            self.critic = Critic(state_dim, action_dim).to(device)\n",
    "            print(\"New Critic network initialized.\")\n",
    "        \n",
    "        # Initialize Critic Target Network\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        if critic is not None:\n",
    "            self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "            print(\"Critic target network initialized with preloaded Critic weights.\")\n",
    "        else:\n",
    "            self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "            print(\"Critic target network initialized with Critic weights.\")\n",
    "        \n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        state = th.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        self.actor.eval()\n",
    "        with th.no_grad():\n",
    "            action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        self.actor.train()\n",
    "        return action\n",
    "    \n",
    "    def train(self, replay_buffer):\n",
    "        if len(replay_buffer) < self.batch_size:\n",
    "            return None, None\n",
    "        \n",
    "        # Sample from replay buffer\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        state = th.FloatTensor(state).to(self.device)\n",
    "        action = th.FloatTensor(action).to(self.device)\n",
    "        reward = th.FloatTensor(reward).reshape(-1, 1).to(self.device)\n",
    "        next_state = th.FloatTensor(next_state).to(self.device)\n",
    "        done = th.FloatTensor(done).reshape(-1, 1).to(self.device)\n",
    "        \n",
    "        # Compute target Q value\n",
    "        with th.no_grad():\n",
    "            target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
    "        \n",
    "        # Get current Q value\n",
    "        current_Q = self.critic(state, action)\n",
    "        \n",
    "        # Compute critic loss\n",
    "        critic_loss = nn.MSELoss()(current_Q, target_Q)\n",
    "        \n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), self.grad_clip)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        \n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(self.actor.parameters(), self.grad_clip)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Soft update target networks\n",
    "        self.soft_update(self.critic, self.critic_target)\n",
    "        self.soft_update(self.actor, self.actor_target)\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "    \n",
    "    def soft_update(self, source, target):\n",
    "        for param, target_param in zip(source.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "def train_ddpg(num_episodes=1000, actor=None, critic=None, device=None, \n",
    "              state_dim=None, action_dim=None, max_action=None):\n",
    "    # Initialize device\n",
    "    if device is None:\n",
    "        device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize environment to get dimensions if not provided\n",
    "    if state_dim is None or action_dim is None or max_action is None:\n",
    "        temp_env = gym.make(\"LunarLanderContinuous-v2\", render_mode=None)\n",
    "        state_dim = temp_env.observation_space.shape[0]\n",
    "        action_dim = temp_env.action_space.shape[0]\n",
    "        max_action = float(temp_env.action_space.high[0])\n",
    "        temp_env.close()\n",
    "    \n",
    "    # Initialize environment with custom reward\n",
    "    env = HoverLunarLander(gym.make(\"LunarLanderContinuous-v2\", render_mode=None))\n",
    "    \n",
    "    # Initialize agent\n",
    "    agent = DDPGAgent(state_dim, action_dim, max_action, device, actor=actor, critic=critic)\n",
    "    replay_buffer = ReplayBuffer()\n",
    "    \n",
    "    # Initialize exploration noise\n",
    "    exploration_noise = OUNoise(action_dim)\n",
    "    exploration_noise.reset()\n",
    "    \n",
    "    # Initialize training logs\n",
    "    training_logs = {\n",
    "        'episode_rewards': [],\n",
    "        'avg_rewards': [],\n",
    "        'actor_losses': [],\n",
    "        'critic_losses': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset(seed=SEED)\n",
    "        exploration_noise.reset()\n",
    "        episode_reward = 0\n",
    "        actor_loss_ep = 0\n",
    "        critic_loss_ep = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(1000):  # max_steps\n",
    "            # Select action and add exploration noise\n",
    "            action = agent.select_action(state)\n",
    "            noise = exploration_noise.sample()\n",
    "            action = action + noise\n",
    "            action = np.clip(action, -agent.max_action, agent.max_action)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition in replay buffer\n",
    "            replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Train agent\n",
    "            actor_loss, critic_loss = agent.train(replay_buffer)\n",
    "            if actor_loss is not None and critic_loss is not None:\n",
    "                actor_loss_ep += actor_loss\n",
    "                critic_loss_ep += critic_loss\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        training_logs['episode_rewards'].append(episode_reward)\n",
    "        if actor_loss_ep != 0 and critic_loss_ep != 0:\n",
    "            training_logs['actor_losses'].append(actor_loss_ep / steps)\n",
    "            training_logs['critic_losses'].append(critic_loss_ep / steps)\n",
    "        else:\n",
    "            training_logs['actor_losses'].append(0)\n",
    "            training_logs['critic_losses'].append(0)\n",
    "        \n",
    "        # Logging every 10 episodes\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = np.mean(training_logs['episode_rewards'][-10:])\n",
    "            avg_actor_loss = np.mean(training_logs['actor_losses'][-10:])\n",
    "            avg_critic_loss = np.mean(training_logs['critic_losses'][-10:])\n",
    "            training_logs['avg_rewards'].append(avg_reward)\n",
    "            print(f\"Episode {episode}\\tAverage Reward: {avg_reward:.2f}\\tAvg Actor Loss: {avg_actor_loss:.4f}\\tAvg Critic Loss: {avg_critic_loss:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent.actor, agent.critic, training_logs\n",
    "\n",
    "def render_episode(actor, state_dim, action_dim, max_action, device=None, render_delay=0.02):\n",
    "    # Initialize device\n",
    "    if device is None:\n",
    "        device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize environment with custom reward\n",
    "    env = HoverLunarLander(gym.make(\"LunarLanderContinuous-v2\", render_mode=\"human\"))\n",
    "    \n",
    "    # Initialize agent with only the actor\n",
    "    agent = DDPGAgent(state_dim, action_dim, max_action, device, actor=actor, critic=None)\n",
    "    \n",
    "    # Initialize environment and get initial state\n",
    "    state, _ = env.reset(seed=SEED)\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done and step_count < 1000:\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        env.render()\n",
    "        step_count += 1\n",
    "        \n",
    "        # Optional: Add a small delay to make rendering visible\n",
    "        # import time\n",
    "        # time.sleep(render_delay)\n",
    "    \n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Actor network initialized.\n",
      "Actor target network initialized with Actor weights.\n",
      "New Critic network initialized.\n",
      "Critic target network initialized with Critic weights.\n",
      "Episode 10\tAverage Reward: -353.27\tAvg Actor Loss: 6.1453\tAvg Critic Loss: 214.1731\n",
      "Episode 20\tAverage Reward: -268.67\tAvg Actor Loss: -3.0546\tAvg Critic Loss: 56.7895\n",
      "Episode 30\tAverage Reward: -183.00\tAvg Actor Loss: -25.8152\tAvg Critic Loss: 37.2655\n",
      "Episode 40\tAverage Reward: -118.24\tAvg Actor Loss: -32.2329\tAvg Critic Loss: 38.5875\n",
      "Episode 50\tAverage Reward: -127.74\tAvg Actor Loss: -38.4952\tAvg Critic Loss: 36.0659\n",
      "Episode 60\tAverage Reward: -212.07\tAvg Actor Loss: -50.6387\tAvg Critic Loss: 38.0034\n",
      "Episode 70\tAverage Reward: -102.94\tAvg Actor Loss: -77.3772\tAvg Critic Loss: 43.0516\n",
      "Episode 80\tAverage Reward: -68.66\tAvg Actor Loss: -83.9638\tAvg Critic Loss: 54.1479\n",
      "Episode 90\tAverage Reward: -99.42\tAvg Actor Loss: -88.8215\tAvg Critic Loss: 51.9274\n",
      "Episode 100\tAverage Reward: -47.30\tAvg Actor Loss: -89.8563\tAvg Critic Loss: 53.8084\n",
      "Episode 110\tAverage Reward: 80.02\tAvg Actor Loss: -89.2559\tAvg Critic Loss: 50.9946\n",
      "Episode 120\tAverage Reward: -6.23\tAvg Actor Loss: -86.5717\tAvg Critic Loss: 47.6145\n",
      "Episode 130\tAverage Reward: 57.35\tAvg Actor Loss: -82.7693\tAvg Critic Loss: 52.4341\n",
      "Episode 140\tAverage Reward: 116.27\tAvg Actor Loss: -78.2083\tAvg Critic Loss: 47.1627\n",
      "Episode 150\tAverage Reward: -19.47\tAvg Actor Loss: -72.3298\tAvg Critic Loss: 35.9586\n",
      "Episode 160\tAverage Reward: -5.60\tAvg Actor Loss: -69.8234\tAvg Critic Loss: 46.8450\n",
      "Episode 170\tAverage Reward: 104.83\tAvg Actor Loss: -67.7567\tAvg Critic Loss: 48.0168\n",
      "Episode 180\tAverage Reward: 124.17\tAvg Actor Loss: -66.7478\tAvg Critic Loss: 39.2832\n",
      "Episode 190\tAverage Reward: 38.95\tAvg Actor Loss: -66.5704\tAvg Critic Loss: 41.2817\n",
      "Episode 200\tAverage Reward: 185.66\tAvg Actor Loss: -66.1700\tAvg Critic Loss: 44.5730\n",
      "Episode 210\tAverage Reward: 103.39\tAvg Actor Loss: -65.8638\tAvg Critic Loss: 54.7157\n",
      "Episode 220\tAverage Reward: 126.81\tAvg Actor Loss: -66.0463\tAvg Critic Loss: 48.3196\n",
      "Episode 230\tAverage Reward: 114.60\tAvg Actor Loss: -66.0596\tAvg Critic Loss: 55.7255\n",
      "Episode 240\tAverage Reward: 149.59\tAvg Actor Loss: -64.5616\tAvg Critic Loss: 49.8825\n",
      "Episode 250\tAverage Reward: 126.84\tAvg Actor Loss: -64.5415\tAvg Critic Loss: 53.2712\n",
      "Episode 260\tAverage Reward: 97.70\tAvg Actor Loss: -65.8654\tAvg Critic Loss: 41.3104\n",
      "Episode 270\tAverage Reward: 123.40\tAvg Actor Loss: -65.9176\tAvg Critic Loss: 44.4774\n",
      "Episode 280\tAverage Reward: 184.68\tAvg Actor Loss: -66.8397\tAvg Critic Loss: 41.0379\n",
      "Episode 290\tAverage Reward: 91.93\tAvg Actor Loss: -66.7575\tAvg Critic Loss: 46.8356\n",
      "Episode 300\tAverage Reward: 57.24\tAvg Actor Loss: -66.7866\tAvg Critic Loss: 43.4487\n",
      "Episode 310\tAverage Reward: 65.48\tAvg Actor Loss: -67.2133\tAvg Critic Loss: 46.5304\n",
      "Episode 320\tAverage Reward: 66.18\tAvg Actor Loss: -67.3312\tAvg Critic Loss: 50.0961\n",
      "Episode 330\tAverage Reward: 175.17\tAvg Actor Loss: -67.0122\tAvg Critic Loss: 52.6888\n",
      "Episode 340\tAverage Reward: 174.04\tAvg Actor Loss: -66.5938\tAvg Critic Loss: 48.6634\n",
      "Episode 350\tAverage Reward: 138.57\tAvg Actor Loss: -66.5451\tAvg Critic Loss: 42.3296\n",
      "Episode 360\tAverage Reward: 193.53\tAvg Actor Loss: -66.0969\tAvg Critic Loss: 52.9048\n",
      "Episode 370\tAverage Reward: 91.16\tAvg Actor Loss: -65.8406\tAvg Critic Loss: 40.9573\n",
      "Episode 380\tAverage Reward: 163.96\tAvg Actor Loss: -65.7791\tAvg Critic Loss: 37.8990\n",
      "Episode 390\tAverage Reward: 160.45\tAvg Actor Loss: -65.4495\tAvg Critic Loss: 43.0027\n",
      "Episode 400\tAverage Reward: 75.86\tAvg Actor Loss: -65.5865\tAvg Critic Loss: 49.9889\n",
      "Episode 410\tAverage Reward: 185.73\tAvg Actor Loss: -66.0092\tAvg Critic Loss: 43.5176\n",
      "Episode 420\tAverage Reward: 146.80\tAvg Actor Loss: -66.1758\tAvg Critic Loss: 41.7200\n",
      "Episode 430\tAverage Reward: 103.18\tAvg Actor Loss: -66.1738\tAvg Critic Loss: 45.7550\n",
      "Episode 440\tAverage Reward: 125.98\tAvg Actor Loss: -66.5587\tAvg Critic Loss: 46.8552\n",
      "Episode 450\tAverage Reward: 200.66\tAvg Actor Loss: -67.0739\tAvg Critic Loss: 45.5920\n",
      "Episode 460\tAverage Reward: 116.23\tAvg Actor Loss: -66.8943\tAvg Critic Loss: 47.5514\n",
      "Episode 470\tAverage Reward: 129.63\tAvg Actor Loss: -67.0396\tAvg Critic Loss: 42.5731\n",
      "Episode 480\tAverage Reward: 201.07\tAvg Actor Loss: -67.2380\tAvg Critic Loss: 44.6928\n",
      "Episode 490\tAverage Reward: 179.40\tAvg Actor Loss: -67.6461\tAvg Critic Loss: 41.0139\n",
      "Episode 500\tAverage Reward: -89.78\tAvg Actor Loss: -68.3195\tAvg Critic Loss: 40.3671\n",
      "Episode 510\tAverage Reward: 41.25\tAvg Actor Loss: -68.9308\tAvg Critic Loss: 56.5600\n",
      "Episode 520\tAverage Reward: 152.70\tAvg Actor Loss: -69.4187\tAvg Critic Loss: 43.1871\n",
      "Episode 530\tAverage Reward: 141.27\tAvg Actor Loss: -69.0441\tAvg Critic Loss: 45.4044\n",
      "Episode 540\tAverage Reward: 115.91\tAvg Actor Loss: -69.1559\tAvg Critic Loss: 50.1165\n",
      "Episode 550\tAverage Reward: 162.46\tAvg Actor Loss: -68.9581\tAvg Critic Loss: 46.2379\n",
      "Episode 560\tAverage Reward: 195.51\tAvg Actor Loss: -68.5237\tAvg Critic Loss: 44.5452\n",
      "Episode 570\tAverage Reward: 218.86\tAvg Actor Loss: -67.9339\tAvg Critic Loss: 49.7003\n",
      "Episode 580\tAverage Reward: 191.53\tAvg Actor Loss: -67.6368\tAvg Critic Loss: 45.6649\n",
      "Episode 590\tAverage Reward: 175.77\tAvg Actor Loss: -67.4519\tAvg Critic Loss: 45.5551\n",
      "Episode 600\tAverage Reward: 190.52\tAvg Actor Loss: -67.0709\tAvg Critic Loss: 39.0680\n",
      "Episode 610\tAverage Reward: 124.45\tAvg Actor Loss: -66.7781\tAvg Critic Loss: 41.2732\n",
      "Episode 620\tAverage Reward: 112.11\tAvg Actor Loss: -66.5763\tAvg Critic Loss: 41.6376\n",
      "Episode 630\tAverage Reward: 196.84\tAvg Actor Loss: -66.8098\tAvg Critic Loss: 46.7354\n",
      "Episode 640\tAverage Reward: 194.80\tAvg Actor Loss: -66.7045\tAvg Critic Loss: 39.3371\n",
      "Episode 650\tAverage Reward: 162.97\tAvg Actor Loss: -66.8863\tAvg Critic Loss: 43.1282\n",
      "Episode 660\tAverage Reward: -12.03\tAvg Actor Loss: -66.9412\tAvg Critic Loss: 39.5897\n",
      "Episode 670\tAverage Reward: 101.21\tAvg Actor Loss: -66.9379\tAvg Critic Loss: 39.2911\n",
      "Episode 680\tAverage Reward: 221.96\tAvg Actor Loss: -67.0672\tAvg Critic Loss: 47.4223\n",
      "Episode 690\tAverage Reward: 226.84\tAvg Actor Loss: -67.4091\tAvg Critic Loss: 46.6125\n",
      "Episode 700\tAverage Reward: 198.12\tAvg Actor Loss: -67.2200\tAvg Critic Loss: 37.3096\n",
      "Episode 710\tAverage Reward: 90.44\tAvg Actor Loss: -67.1822\tAvg Critic Loss: 41.0867\n",
      "Episode 720\tAverage Reward: 114.92\tAvg Actor Loss: -67.3443\tAvg Critic Loss: 42.1001\n",
      "Episode 730\tAverage Reward: 172.21\tAvg Actor Loss: -67.2483\tAvg Critic Loss: 35.8210\n",
      "Episode 740\tAverage Reward: 196.57\tAvg Actor Loss: -67.0304\tAvg Critic Loss: 43.7542\n",
      "Episode 750\tAverage Reward: 45.47\tAvg Actor Loss: -67.2790\tAvg Critic Loss: 41.8240\n",
      "Episode 760\tAverage Reward: 101.33\tAvg Actor Loss: -67.4819\tAvg Critic Loss: 36.6336\n",
      "Episode 770\tAverage Reward: 71.99\tAvg Actor Loss: -67.6228\tAvg Critic Loss: 51.7845\n",
      "Episode 780\tAverage Reward: -30.52\tAvg Actor Loss: -67.6441\tAvg Critic Loss: 41.9960\n",
      "Episode 790\tAverage Reward: 41.45\tAvg Actor Loss: -67.4172\tAvg Critic Loss: 40.9397\n",
      "Episode 800\tAverage Reward: -70.45\tAvg Actor Loss: -67.1880\tAvg Critic Loss: 35.3592\n",
      "Episode 810\tAverage Reward: -33.47\tAvg Actor Loss: -67.5858\tAvg Critic Loss: 43.6561\n",
      "Episode 820\tAverage Reward: 78.70\tAvg Actor Loss: -68.3058\tAvg Critic Loss: 48.9850\n",
      "Episode 830\tAverage Reward: 106.73\tAvg Actor Loss: -68.7710\tAvg Critic Loss: 49.0935\n",
      "Episode 840\tAverage Reward: 163.32\tAvg Actor Loss: -68.9065\tAvg Critic Loss: 48.7808\n",
      "Episode 850\tAverage Reward: 224.10\tAvg Actor Loss: -69.2475\tAvg Critic Loss: 47.7393\n",
      "Episode 860\tAverage Reward: 177.14\tAvg Actor Loss: -69.4613\tAvg Critic Loss: 44.4535\n",
      "Episode 870\tAverage Reward: 106.65\tAvg Actor Loss: -69.5507\tAvg Critic Loss: 45.9192\n",
      "Episode 880\tAverage Reward: 128.22\tAvg Actor Loss: -69.2072\tAvg Critic Loss: 47.2922\n",
      "Episode 890\tAverage Reward: 110.64\tAvg Actor Loss: -68.7877\tAvg Critic Loss: 40.1525\n",
      "Episode 900\tAverage Reward: 88.79\tAvg Actor Loss: -68.7245\tAvg Critic Loss: 43.7191\n",
      "Episode 910\tAverage Reward: 129.80\tAvg Actor Loss: -68.4703\tAvg Critic Loss: 40.3118\n",
      "Episode 920\tAverage Reward: 51.89\tAvg Actor Loss: -69.0236\tAvg Critic Loss: 32.2793\n",
      "Episode 930\tAverage Reward: 193.16\tAvg Actor Loss: -68.9892\tAvg Critic Loss: 49.2952\n",
      "Episode 940\tAverage Reward: 194.98\tAvg Actor Loss: -68.8285\tAvg Critic Loss: 54.7357\n",
      "Episode 950\tAverage Reward: 187.69\tAvg Actor Loss: -69.2985\tAvg Critic Loss: 46.8608\n",
      "Episode 960\tAverage Reward: 134.38\tAvg Actor Loss: -69.2973\tAvg Critic Loss: 43.3248\n",
      "Episode 970\tAverage Reward: 111.90\tAvg Actor Loss: -69.1693\tAvg Critic Loss: 45.0962\n",
      "Episode 980\tAverage Reward: -83.12\tAvg Actor Loss: -69.5974\tAvg Critic Loss: 50.9553\n",
      "Episode 990\tAverage Reward: -75.32\tAvg Actor Loss: -69.9608\tAvg Critic Loss: 39.7816\n",
      "Episode 1000\tAverage Reward: 118.40\tAvg Actor Loss: -69.8858\tAvg Critic Loss: 47.1176\n"
     ]
    }
   ],
   "source": [
    "actor, critic, training_logs = train_ddpg(num_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# file_path = 'traininglog.json'\n",
    "# with open(file_path, 'w') as f:\n",
    "#     json.dump(training_logs, f, indent=4) # indent for pretty-printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "# state_dim = temp_env.observation_space.shape[0]\n",
    "# action_dim = temp_env.action_space.shape[0]\n",
    "# max_action = float(temp_env.action_space.high[0])\n",
    "# actor = Actor(state_dim, action_dim, max_action)\n",
    "# actor.load_state_dict(th.load('actor_custom_reward.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save actor critic, custom reward name\n",
    "\n",
    "# th.save(actor.state_dict(), 'actor_normal_reward.pth')\n",
    "# th.save(critic.state_dict(), 'critic_normal_reward.pth')\n",
    "\n",
    "# #plotting\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.plot(training_logs['episode_rewards'], label='Episode Reward')\n",
    "# plt.plot(training_logs['avg_rewards'], label='Average Reward (10 episodes)')\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Reward')\n",
    "# plt.title('DDPG Training on HoverLunarLander')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#render episode\n",
    "#render_episode(actor, state_dim=8, action_dim=2, max_action=1.0, device=None, render_delay=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Render Gif Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloaded Actor network loaded.\n",
      "Actor target network initialized with preloaded Actor weights.\n",
      "New Critic network initialized.\n",
      "Critic target network initialized with Critic weights.\n",
      "Rendered GIF saved as rendered_episode_normal.gif\n"
     ]
    }
   ],
   "source": [
    "def render_episode_gif(actor, state_dim, action_dim, max_action, device=None, render_delay=0.02):\n",
    "\n",
    "    # Initialize device\n",
    "    if device is None:\n",
    "        device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize environment with custom reward and render_mode='rgb_array'\n",
    "    env = HoverLunarLander(gym.make(\"LunarLanderContinuous-v2\", render_mode='rgb_array'))\n",
    "    \n",
    "    # Initialize agent with only the actor\n",
    "    agent = DDPGAgent(state_dim, action_dim, max_action, device, actor=actor, critic=None)\n",
    "    \n",
    "    # Initialize environment and get initial state\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    images = []  # To store frames\n",
    "    \n",
    "    while not done and step_count < 250:\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        frame = env.render()  # Removed mode='rgb_array'\n",
    "        images.append(frame)\n",
    "        step_count += 1\n",
    "        \n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    gif_path = 'rendered_episode_normal.gif'\n",
    "    imageio.mimsave(gif_path, images, fps=30, loop=0)\n",
    "    print(f\"Rendered GIF saved as {gif_path}\")\n",
    "\n",
    "render_episode_gif(actor, state_dim=8, action_dim=2, max_action=1.0, device=None, render_delay=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
